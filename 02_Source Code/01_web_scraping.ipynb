{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![iCor logo light](../logo.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iCOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iCOR is a company that builds legal registers for companies to evaluate their environmental performance. They do this by manually searching environmental legislation and summarising it along with steps of what has changed. \n",
    "\n",
    "At the moment iCOR do this process by hand. They would like to be able to automate this. Can we develop a business idea around this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting Legislation Content\n",
    "\n",
    "This notebook will take you through the process of scrapping information from a webpage and extracting the relevant information. Here we are going to focus on legislation from [legislation.gov.uk](https://www.legislation.gov.uk/). To start with we will focus on the legislation for the [Hazardous Waste Regulations](https://www.legislation.gov.uk/uksi/2005/894/contents)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to import libraries. We will be using the packages `bs4` and `request` for web scraping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For web scrapping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#Â For text manipulation\n",
    "import re\n",
    "\n",
    "# To create a dataframe\n",
    "import pandas as pd\n",
    "\n",
    "# To create directories\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scraping\n",
    "\n",
    "We will make a extract the information for the the [Hazardous Waste Regulations](https://www.legislation.gov.uk/uksi/2005/894/contents). At the moment this webpage contains a lot of links to other webpages, however we can use the `Print Options` dropdown to have the whole information from each additional webpage printed onto [one webpage](https://www.legislation.gov.uk/uksi/2005/894/data.xht?view=snippet&wrap=true). We will use this webpage instead.\n",
    "\n",
    " We will scrape the information using the `requests` package to first make a request to the web page, and then use the `get()` method to extract the content of the webpage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.legislation.gov.uk/uksi/2005/894/data.xht?view=snippet&wrap=true\"\n",
    "res = requests.get(url)\n",
    "html_data = res.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can use `BeautifulSoup` to parse the plain text from `html_data` into an html format which is often referred to as a tree. We can then make use of the many methods `BeautifulSoup` has to offer to extract information and navigate through the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<!DOCTYPE html\n",
      "  PUBLIC \"-//W3C//DTD XHTML+RDFa 1.0//EN\" \"http://www.w3.org/MarkUp/DTD/xhtml-rdfa-1.dtd\">\n",
      "<html xml:lang=\"en\" xmlns=\"http://www.w3.org/1999/xhtml\">\n",
      " <head>\n",
      "  <meta content=\"text/html; charset=utf-8\" http-equiv=\"Content-Type\"/>\n",
      "  <title xmlns:atom=\"http://www.w3.org/2005/Atom\">\n",
      "   The Hazardous Waste (England and Wales)Regulations 2005 No. 894\n",
      "  </title>\n",
      "  xmlns:atom=\"http://www.w3.org/2005/Atom\"\n",
      "  <meta content=\"2024-05-16\" name=\"DC.Date.Modified\" scheme=\"W3CDTF\"/>\n",
      "  xmlns:atom=\"http://www.w3.org/2005/Atom\"\n",
      "  <meta content=\"2020-12-31\" name=\"DC.Date.Valid\" scheme=\"W3CDTF\"/>\n",
      "  <style media=\"screen, print\" type=\"text/css\" xmlns:atom=\"http://www.w3.org/2005/Atom\">\n",
      "   @import \"/styles/legislation.css\";@import \"/styles/secondarylegislation.css\";&#xD;\n",
      "  </style>\n",
      " </head>\n",
      " <body>\n",
      "  <div class=\"LegSnippet\">\n",
      "   <a name=\"content\">\n",
      "   </a>\n",
      "   <div class=\"DocContainer\" xmlns:atom=\"http://www.w3.org/2005/Atom\">\n",
      "    <div class=\"LegClearFix LegPrelims\">\n",
      "     <p class=\"LegBanner\">\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(html_data, \"html.parser\") # parsing html data so goes into the form of html instead of plain text.\n",
    "print(soup.prettify(formatter=\"html\")[0:1000]) # restrict print-out to first 1000 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the document in the form of a tree we can use tags to find specific content by using either the `find()` method to find only the first instance of the tag, or the `find_all()` method to find all instances of the tag. Tags also often have addition information we can filter on, such as classes and ids. We can use these to pass addition information to the filter and pinpoint the specific content we want to extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statutory Instruments\n",
      "ENVIRONMENTAL PROTECTION,ENGLAND AND WALES\n",
      "Made\n",
      "23rd March 2005\n",
      "Laid before Parliament\n",
      "24th March 2005\n",
      "Coming into force in accordance with regulation 1(1)\n",
      "The Secretary of State, being a Minister designated M1 for the purposes of section 2(2) of the European Communities Act 1972 M2 in relation to measures relating to the prevention, reduction and elimination of pollution caused by waste, in exercise of the powers conferred on her by section 2(2) of that Act and section 156 of the Environmental Protection Act 1990 M3, makes the following Regulations: \n",
      "Marginal Citations\n",
      "M1S.I. 1992/2870. The National Assembly for Wales is designated in relation to the controlled management of hazardous waste in Wales (see S.I. 2001/3495). The designations in relation to waste for National Assembly for Wales are shortly to be brought into line with those of the Secretary of State.\n",
      "M21972 c. 68.\n",
      "M31990 c. 43. The relevant functions of the Secretary of State in so far as they relate \n"
     ]
    }
   ],
   "source": [
    "content = soup.find_all('p')                            # extracting all paragraphs\n",
    "text_content = [c.text for c in content]                # extracting the plain text from the content\n",
    "text_content = '\\n'.join(text_content)                   # joining the text together\n",
    "for string in [\"\\t\", \"\\r\", \"\\xa0\"]:                     # removing unwanted characters such as \\t for tab.\n",
    "    text_content = text_content.replace(string, \" \")\n",
    "text_content = re.sub(' +', ' ', text_content)           # removing multiple spaces\n",
    "\n",
    "print(text_content[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "132462"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have _a lot_ of text, with around 132,500 characters! This isn't too surprising since the webpage we are scraping is very long.\n",
    "\n",
    "Alternatively, if we are unsure on where the main body of text resides within the large html tree, then we can use the `get_text()` method to extract all text from the tree. This is fine for our use case _but_ on other webpages this will extract text such as the names of navigation tabs, any information at the bottom of the webpage, and other unneeded information. We also loose all of the structure of the webpage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Hazardous Waste (England and Wales)Regulations 2005 No. 894 xmlns:atom=\"http://www.w3.org/2005/Atom\" xmlns:atom=\"http://www.w3.org/2005/Atom\" Statutory Instruments 2005 No. 894 ENVIRONMENTAL PROTECTION,ENGLAND AND WALES The Hazardous Waste (England and Wales)Regulations 2005 Made 23rd March 2005 Laid before Parliament 24th March 2005 Coming into force in accordance with regulation 1(1) The Secretary of State, being a Minister designated M1 for the purposes of section 2(2) of the European Communities Act 1972 M2 in relation to measures relating to the prevention, reduction and elimination of pollution caused by waste, in exercise of the powers conferred on her by section 2(2) of that Act and section 156 of the Environmental Protection Act 1990 M3 , makes the following Regulations: Marginal Citations M1 S.I. 1992/2870 . The National Assembly for Wales is designated in relation to the controlled management of hazardous waste in Wales (see S.I . 2001/3495). The designations in relatio\n"
     ]
    }
   ],
   "source": [
    "all_text = soup.get_text(separator= ' ')                      # extracting all of the text from the tree\n",
    "for string in [\"\\n\",\"\\t\", \"\\r\", \"\\xa0\"]:                   # removing unwanted characters such as \\t for tab.\n",
    "    all_text = all_text.replace(string, \" \")        \n",
    "all_text = re.sub(' +', ' ', all_text)                          # removing multiple spaces\n",
    "\n",
    "print(all_text[0:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140744"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see this time we extract more text. We have an approximately 8000 extra characters.\n",
    "\n",
    "We can then save this text along with the relevant url that it came from into a csv file. We will also extract the title of the legislation so we know what legislation the text is referring to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Hazardous Waste (England and Wales)Regulations 2005 No. 894\n"
     ]
    }
   ],
   "source": [
    "content = soup.find('title')\n",
    "title = content.get_text()\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Hazardous Waste (England and Wales)Regulat...</td>\n",
       "      <td>https://www.legislation.gov.uk/uksi/2005/894/d...</td>\n",
       "      <td>The Hazardous Waste (England and Wales)Regula...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  The Hazardous Waste (England and Wales)Regulat...   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.legislation.gov.uk/uksi/2005/894/d...   \n",
       "\n",
       "                                                text  \n",
       "0   The Hazardous Waste (England and Wales)Regula...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dictionary with the information where the keys are the column names and the values are the information, i.e. 'column_name':[info].\n",
    "info_for_df = {'title':[title], 'url':[url], 'text':[all_text]} \n",
    "\n",
    "# create a dataframe from the dictionary\n",
    "df = pd.DataFrame(info_for_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataframe\n",
    "Path('data/raw').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df.to_csv('data/raw/legislation.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be done for many of the legislation pages and they can be subsequently added to the table. Try to find other links to environmental legislation that we could apply this to. We can then build up the CSV file with other legislation that we can then use in `02-text_summarisation.ipynb`.\n",
    "\n",
    "To do this we need to:\n",
    "1. read in the current csv file with `df = pd.read_csv(...)`\n",
    "2. perform the web scraping on the url\n",
    "3. add a new row to the dataframe with this new information\n",
    "4. repeat steps 2 and 3 until you have extracted all of the legislation that you want\n",
    "5. save the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
